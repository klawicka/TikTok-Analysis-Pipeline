{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from statistics import mean\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#langauge detection\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having files generated with the speech-to-text program I make them all into one csv file \n",
    "including the TikTok ID and user ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('speech-to-text-generated.csv').drop(\"Unnamed: 0\", axis=1)\n",
    "df = df.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code sinpets from:\n",
    "\n",
    "https://rafalrolczynski.com/2021/03/07/aspect-based-sentiment-analysis/\n",
    "https://github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis\n",
    "https://towardsdatascience.com/nlp-project-with-augmentation-attacks-aspect-based-sentiment-analysis-3342510c90e7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/nlplanet/quick-intro-to-aspect-based-sentiment-analysis-c8888a09eda7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kinga\\anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load Aspect-Based Sentiment Analysis model\n",
    "absa_tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\")\n",
    "absa_model = AutoModelForSequenceClassification \\\n",
    "  .from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\")\n",
    "\n",
    "# Load a traditional Sentiment Analysis model\n",
    "sentiment_model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "sentiment_model = pipeline(\"sentiment-analysis\", model=sentiment_model_path,\n",
    "                          tokenizer=sentiment_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob = pd.DataFrame(columns=[\"negative\", \"neutral\", \"positive\", \"text\",\"id\"])\n",
    "\n",
    "df['col_isalphanumeric'] = df[\"col\"].str.isspace()\n",
    "df = df[df['col_isalphanumeric'] ==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Sentiment Analysis works best for English so I filter out other languges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect language\n",
    "lang_list = []\n",
    "i = 0\n",
    "for text in df[\"col\"]:\n",
    "    try: \n",
    "        lang_list.append(detect(text))\n",
    "    except:\n",
    "        lang_list.append(\"Nan\")\n",
    "\n",
    "df[\"language\"] = lang_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['language'] =='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_aspect_chunks(text,aspect):\n",
    "    chunk_list = []\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        if sent.text.find(aspect)!= -1:\n",
    "            chunk_list.append(sent.text)\n",
    "    return chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentiment_aspect_df(aspect, df):\n",
    "    df_prob = pd.DataFrame(columns=[\"negative\", \"neutral\", \"positive\", \n",
    "                                    \"text\", \"aspect\", \"video_id\", \"vodeo_user_id\"])\n",
    "\n",
    "    positive_sentiment = []\n",
    "    negative_sentiment = []\n",
    "    neutral_sentiment = []\n",
    "    sentence_list = []\n",
    "    \n",
    "\n",
    "    df = df[df[\"col\"].str.find(aspect) != -1]\n",
    "    \n",
    "    for sentence_one in df[\"col\"]:\n",
    "        text = string_aspect_chunks(sentence_one,aspect) \n",
    "        positive = []\n",
    "        negative = []\n",
    "        neutral = [] \n",
    "        score = [] \n",
    "        for sentence in text:\n",
    "            sentence_list.append(sentence)\n",
    "            inputs = absa_tokenizer(f\"[CLS] {sentence} [SEP] {aspect} [SEP]\", return_tensors=\"pt\")\n",
    "            outputs = absa_model(**inputs)\n",
    "            probs = F.softmax(outputs.logits, dim=1)\n",
    "            probs = probs.detach().numpy()[0]\n",
    "            for prob, label in zip(probs, [\"negative\", \"neutral\", \"positive\"]):\n",
    "                if label == \"negative\":\n",
    "                    negative.append(prob)\n",
    "                if label == \"neutral\":\n",
    "                    neutral.append(prob)\n",
    "                if label == \"positive\":\n",
    "                    positive.append(prob)\n",
    "                \n",
    "        positive_sentiment.append(mean(positive))\n",
    "        negative_sentiment.append(mean(negative))\n",
    "        neutral_sentiment.append(mean(neutral))\n",
    "                                 \n",
    "    df_prob[\"video_id\"] = df[\"video_id\"]\n",
    "    df_prob[\"negative\"] = negative_sentiment\n",
    "    df_prob[\"positive\"] = positive_sentiment\n",
    "    df_prob[\"neutral\"] = neutral_sentiment\n",
    "    df_prob[\"text\"] = df[\"col\"]\n",
    "    df_prob[\"vodeo_user_id\"] = df[\"vodeo_user_id\"]\n",
    "    \n",
    "\n",
    "\n",
    "    df_prob[\"aspect\"]= aspect\n",
    "    return df_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my study was on Russian Propaganda but you can create your own list or import it \n",
    "aspect_list = [\"Volodymyr\", \"Zelenskyy\", \"Ukrain\",\"Russia\", \"Vladimir\", \"Putin\",\n",
    "                \"nato\", \"war\", \"military\", \"politi\", \"west\", \"east\", \"Poland\",\n",
    "                \"Dutch\", \"Polish\", \"Netherlands\", \"US\", \"America\", \"United States\", \"EU\", \"Europe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat((make_sentiment_aspect_df(aspect, df) for aspect in aspect_list ), ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "405ea56bf45ea199fac477ac372722b0c5ca4ba0ab21c322acc84a9de2bce2a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
